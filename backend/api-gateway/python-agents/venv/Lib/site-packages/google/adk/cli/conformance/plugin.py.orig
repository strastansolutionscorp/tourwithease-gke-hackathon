# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Conformance test plugin for ADK agents."""

from __future__ import annotations

import logging
import time
from typing import Any
from typing import Literal
from typing import Optional
from typing import TYPE_CHECKING

from google.genai import types
from typing_extensions import override

from ...agents.callback_context import CallbackContext
from ...models.llm_request import LlmRequest
from ...models.llm_response import LlmResponse
from ...plugins.base_plugin import BasePlugin
from ...utils.yaml_utils import dump_pydantic_to_yaml
from .test_case import LlmRequestRecord
from .test_case import LlmResponseRecord
from .test_case import Recordings
from .test_case import ToolCallRecord
from .test_case import ToolResultRecord

if TYPE_CHECKING:
  from ...agents.invocation_context import InvocationContext
  from ...tools.base_tool import BaseTool
  from ...tools.tool_context import ToolContext

logger = logging.getLogger("google_adk." + __name__)


class RecordingsPlugin(BasePlugin):
  """Plugin for ADK Conformance Tests."""

  def __init__(
      self,
      *,
      test_case_path: str,
      invocation_index: int,
      mode: Literal["record", "replay"] = "record",
  ) -> None:
    """Initialize the conformance test plugin.

    Args:
      test_case_path: Path to test case file
      invocation_index: Current invocation index (0-based)
      mode: "record" or "replay" mode
    """
    super().__init__(name="adk_recordings")
    self._test_case_path = test_case_path
    self._invocation_index = invocation_index
    self._mode = mode

    # Load or create recordings
    self._records = self._load_or_create_recordings()

    if self._mode == "replay":
      # TODO: Implement replay mode
      raise NotImplementedError("Replay mode not yet implemented")

  def _load_or_create_recordings(self) -> Recordings:
    """Load existing recordings if they exist, otherwise create new ones."""
    from pathlib import Path

    import yaml

    recordings_file = Path(self._test_case_path) / "generated-recordings.yaml"

    if recordings_file.exists() and self._mode == "record":
      # Load existing recordings to append to them
      try:
        with open(recordings_file, "r", encoding="utf-8") as f:
          data = yaml.safe_load(f)
        records = Recordings.model_validate(data)
        logger.debug(
            "Loaded existing recordings with %d timeline records",
            len(records.timeline),
        )
        return records
      except Exception as e:
        logger.warning(
            "Failed to load existing recordings: %s, starting fresh", e
        )
        return Recordings()
    else:
      return Recordings()

  @override
  async def before_model_callback(
      self, *, callback_context: CallbackContext, llm_request: LlmRequest
  ) -> Optional[LlmResponse]:
    """Record LLM request."""
    if self._mode == "record":
      request_record = LlmRequestRecord(
          timestamp=time.time(),
          agent_name=callback_context.agent_name,
          invocation_index=self._invocation_index,
          request=llm_request,
      )
      self._records.timeline.append(request_record)

      logger.debug(
          "Recorded LLM request for agent %s: model=%s, contents=%d",
          callback_context.agent_name,
          llm_request.model,
          len(llm_request.contents),
      )

    # TODO: Implement replay mode validation and response
    return None

  @override
  async def after_model_callback(
      self, *, callback_context: CallbackContext, llm_response: LlmResponse
  ) -> Optional[LlmResponse]:
    """Record LLM response."""
    if self._mode == "record":
      response_record = LlmResponseRecord(
          timestamp=time.time(),
          agent_name=callback_context.agent_name,
          invocation_index=self._invocation_index,
          response=llm_response,
      )
      self._records.timeline.append(response_record)

      logger.debug(
          "Recorded LLM response for agent %s",
          callback_context.agent_name,
      )

    return None

  @override
  async def before_tool_callback(
      self,
      *,
      tool: BaseTool,
      tool_args: dict[str, Any],
      tool_context: ToolContext,
  ) -> Optional[dict]:
    """Record tool call."""
    if self._mode == "record":
      # Create FunctionCall from tool context
      function_call = types.FunctionCall(
          id=tool_context.function_call_id, name=tool.name, args=tool_args
      )

      call_record = ToolCallRecord(
          timestamp=time.time(),
          agent_name=tool_context.agent_name,
          invocation_index=self._invocation_index,
          function_call=function_call,
      )
      self._records.timeline.append(call_record)

      logger.debug(
          "Recorded tool call for agent %s: tool=%s, id=%s",
          tool_context.agent_name,
          tool.name,
          tool_context.function_call_id,
      )

    return None

  @override
  async def after_tool_callback(
      self,
      *,
      tool: BaseTool,
      tool_args: dict[str, Any],
      tool_context: ToolContext,
      result: dict,
  ) -> Optional[dict]:
    """Record tool result."""
    if self._mode == "record":
      # Create FunctionResponse from tool result
      function_response = types.FunctionResponse(
          id=tool_context.function_call_id,
          name=tool.name,
          response={"result": result},
      )

      result_record = ToolResultRecord(
          timestamp=time.time(),
          agent_name=tool_context.agent_name,
          invocation_index=self._invocation_index,
          function_response=function_response,
      )
      self._records.timeline.append(result_record)

      logger.debug(
          "Recorded tool result for agent %s: tool=%s, id=%s",
          tool_context.agent_name,
          tool.name,
          tool_context.function_call_id,
      )

    return None

  @override
  async def on_tool_error_callback(
      self,
      *,
      tool: BaseTool,
      tool_args: dict[str, Any],
      tool_context: ToolContext,
      error: Exception,
  ) -> Optional[dict]:
    """Record tool error."""
    if self._mode == "record":
      # Create FunctionResponse with error
      function_response = types.FunctionResponse(
          id=tool_context.function_call_id,
          name=tool.name,
          response=None,  # No response data on error
      )

      result_record = ToolResultRecord(
          timestamp=time.time(),
          agent_name=tool_context.agent_name,
          invocation_index=self._invocation_index,
          function_response=function_response,
          error=str(error),
      )
      self._records.timeline.append(result_record)

      logger.debug(
          "Recorded tool error for agent %s: tool=%s, id=%s, error=%s",
          tool_context.agent_name,
          tool.name,
          tool_context.function_call_id,
          str(error),
      )

    return None

  @override
  async def after_run_callback(
      self, *, invocation_context: InvocationContext
  ) -> None:
    """Save recorded interactions to recordings file."""
    if self._mode == "record":
      try:
        dump_pydantic_to_yaml(
            self._records,
            f"{self._test_case_path}/generated-recordings.yaml",
            sort_keys=False,
        )
        logger.info(
            "Saved %d timeline records to %s/generated-recordings.yaml",
            len(self._records.timeline),
            self._test_case_path,
        )
      except Exception as e:
        logger.error(
            "Failed to save interactions: %s",
            e,
        )


# TODO: Implement a new plugin for replay mode:
# - Validate the SI and content text list in the outgoing LlmRequest
# - Feedback the pre-recorded LlmResponse and tool result.
# - Raise error if any record is not used in an invocaiton.
